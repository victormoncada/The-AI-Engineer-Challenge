{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Use Case Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Project Title\",\n",
        "      \"Project Domain\",\n",
        "      \"Secondary Domain\",\n",
        "      \"Description\",\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthetic_usecase_data = loader.load()\n",
        "\n",
        "for doc in synthetic_usecase_data:\n",
        "    doc.page_content = doc.metadata[\"Description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/Projects_with_Domains.csv', 'row': 0, 'Project Title': 'InsightAI 1', 'Project Domain': 'Security', 'Secondary Domain': 'Finance / FinTech', 'Description': 'A low-latency inference system for multimodal agents in autonomous systems.', 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='A low-latency inference system for multimodal agents in autonomous systems.')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synthetic_usecase_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"Synthetic_Usecases\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    synthetic_usecase_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecases\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" which is mentioned multiple times across different projects.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, one project titled \"LatticeFlow\" in the Healthcare / MedTech domain has a secondary focus on Security. The project involves an AI-powered platform optimizing logistics routes for sustainability, with a note indicating good potential for scalability and commercialization.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges generally had positive comments about the fintech-related projects, noting their technical ambition, robustness, and real-world impact. For example, one project was described as \"Technically ambitious and well-executed,\" while another was praised for being \"Promising\" with a \"robust experimental validation.\" Additionally, a project was recognized as a \"solid work with impressive real-world impact.\"'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(synthetic_usecase_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain is not explicitly stated. However, in the sample data, the domains mentioned are Productivity Assistants, Legal / Compliance, Data / Analytics, and Healthcare / MedTech. To determine the most common domain, a larger dataset would be needed. \\n\\nGiven only the sample, I cannot definitively state which is most common. If you have more data or specific counts, I can help analyze them further.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases related to security mentioned.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The judges' comments about the fintech projects indicate that they found such projects to be technically ambitious and well-executed.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer\n",
        "\n",
        "\"What is the API rate limit for GPT-4?\"\n",
        "Why? Because the word matching is very specific, API rate is super specific to that and we can expect an exact number, therefore, BM25 will prioritize the exact terms \"rate\" \"API\" \"GPT-4\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Creative / Design / Media,\" as it is listed alongside other projects in the dataset. However, since the sample includes only a few entries, I cannot definitively determine the most common domain overall. \\n\\nIf these are representative samples, then \"Creative / Design / Media\" is prominently listed. \\n\\nPlease let me know if you\\'d like a more detailed analysis or additional data!'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific usecases related to security. The examples primarily focus on federated learning tools aimed at improving privacy in healthcare applications, but do not explicitly mention security usecases.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had generally positive remarks about the fintech projects. For example, they praised the project \"Pathfinder 27\" for its excellent code quality and effective use of open-source libraries.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is \"Healthcare / MedTech,\" which appears multiple times among the projects listed.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security mentioned in the provided context. Specifically, one project titled \"Project Aurora\" with the project domain \"Security\" focuses on a low-latency inference system for multimodal agents in autonomous systems.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had a generally positive view of the fintech projects, with some highlighting their strong code quality, promising ideas, and real-world impact. For example, the project \"Pathfinder\" received a high judge score of 9.8 and was praised for excellent code quality and use of open-source libraries. Similarly, \"DataWeave\" was noted for its excellent quality with a judge score of 9.6. However, some projects, like \"SecureNest 28,\" were recognized as conceptually strong but needing more benchmarking results. Overall, judges appreciated the innovative solutions and their potential, though some emphasized the need for further validation or benchmarking to substantiate claims.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer\n",
        "More relevant documents found (higher recall) at the cost of more API calls.\n",
        "Think of it as: One question, multiple angles = better coverage ðŸŽ¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = synthetic_usecase_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain is not explicitly stated, but from the examples given, the project domains include Healthcare / MedTech, Productivity Assistants, Creative / Design / Media, and Security. Since only a few entries are shown, I cannot determine definitively which is the most common overall. However, if you have access to the full dataset, counting the frequency of each domain would reveal the most common one.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases related to security mentioned in the projects. The projects focus on federated learning to improve privacy, particularly in healthcare applications, but there are no explicit references to security use cases.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had positive comments about the fintech projects. Specifically, they described some of the solutions as \"a clever solution with measurable environmental benefit,\" \"comprehensive and technically mature,\" \"promising idea with robust experimental validation,\" and \"technically ambitious and well-executed.\"'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" as it is mentioned multiple times among the projects listed.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. Specifically, there is a project titled \"MediMind 17\" with the description: \"A medical imaging solution improving early diagnosis through vision transformers,\" which falls under the Security domain.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had generally positive comments about the fintech projects. For example, one project, \"Pathfinder 27,\" was praised for its excellent code quality and use of open-source libraries, receiving a judge score of 9.8. Another project, \"SecureNest 28,\" was recognized for being conceptually strong, although it needed more benchmarking, and received a judge score of 9.0. Additionally, \"DocuCheck 47\" was noted for its strong conceptual foundation and scored 9.6 from judges. Overall, the feedback indicates that the judges appreciated the quality, concept strength, and potential of the fintech projects.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(synthetic_usecase_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecase_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Legal / Compliance,\" which is mentioned twice. Other domains like \"Developer Tools / DevEx,\" \"Customer Support / Helpdesk,\" and \"Writing & Content\" are also repeated, but less frequently. \\n\\nSince the data sample is limited, and \"Legal / Compliance\" is the only domain that appears more than once in this set, I conclude that \"Legal / Compliance\" is the most common project domain in this context.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, one project titled \"SynthMind\" falls under the Security domain. It involves a medical imaging solution that leverages vision transformers for early diagnosis, and it has been recognized for exceeding expectations in creativity and usability. Additionally, another project titled \"Project Aurora\" (also within Security) focuses on a low-latency inference system for multimodal agents in autonomous systems.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had various comments about the fintech projects. For example:\\n\\n- The project \"WealthifyAI 16\" was described as having a \"comprehensive and technically mature approach.\"\\n- \"TrendLens 19\" was noted for being \"technically ambitious and well-executed.\"\\n- \"AutoMate 5\" was recognized as \"a forward-looking idea with solid supporting data.\"\\n- \"InsightAI 1\" was praised for its \"great clarity in communication and demo flow.\"\\n\\nOverall, judges appreciated the technical ambition, maturity, and clarity of the fintech-related projects, highlighting their innovation and solid execution.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### Step 1: Install Required Dependencies and Set Up LangSmith\n",
        "\n",
        "#First, we need to ensure we have Ragas installed and configure LangSmith for tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ragas if not already installed\n",
        "# !pip install ragas langsmith\n",
        "\n",
        "# Set up LangSmith tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"09_Advanced_Retrieval_Evaluation\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Generate Golden Dataset with Ragas\n",
        "\n",
        "We'll use Ragas to generate a synthetic test dataset with questions, ground truths, and contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ragas.testset.generator'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtestset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestsetGenerator\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtestset\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple, reasoning, multi_context\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ragas.testset.generator'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# For Ragas 0.3.x, we'll create a golden dataset manually\n",
        "# This represents typical queries users might ask about the project data\n",
        "\n",
        "golden_dataset = [\n",
        "    {\n",
        "        \"user_input\": \"What is the most common project domain?\",\n",
        "        \"reference\": \"Healthcare / MedTech is the most common project domain in the dataset.\",\n",
        "        \"reference_contexts\": [\"Projects with Healthcare / MedTech domain appear multiple times across different projects.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"Were there any projects about security?\",\n",
        "        \"reference\": \"Yes, there are security-related projects including Project Aurora which focuses on low-latency inference systems for multimodal agents in autonomous systems.\",\n",
        "        \"reference_contexts\": [\"Project Aurora with project domain Security focuses on a low-latency inference system for multimodal agents in autonomous systems.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"What did judges say about the fintech projects?\",\n",
        "        \"reference\": \"Judges provided positive feedback on fintech projects, noting technical innovation and practical applications in the financial technology sector.\",\n",
        "        \"reference_contexts\": [\"Judge comments on fintech projects highlight technical ambition, practical applications, and innovative approaches to financial technology.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"Which projects received the highest scores?\",\n",
        "        \"reference\": \"Projects with scores in the 85-95 range received the highest ratings from judges.\",\n",
        "        \"reference_contexts\": [\"Top-scoring projects achieved scores between 85-95, with judge scores ranging from 8.5 to 9.5.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"Are there any AI projects focused on healthcare?\",\n",
        "        \"reference\": \"Yes, there are multiple healthcare-focused AI projects in the dataset, covering various medical technology applications.\",\n",
        "        \"reference_contexts\": [\"Several projects have Healthcare / MedTech as their primary domain, focusing on AI applications in medical settings.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"What types of multimodal agent systems are described?\",\n",
        "        \"reference\": \"The dataset includes multimodal agent systems for autonomous systems with low-latency inference capabilities.\",\n",
        "        \"reference_contexts\": [\"InsightAI 1 describes a low-latency inference system for multimodal agents in autonomous systems.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"Which secondary domains are represented?\",\n",
        "        \"reference\": \"Secondary domains include Finance / FinTech among others, showing interdisciplinary project approaches.\",\n",
        "        \"reference_contexts\": [\"Projects have secondary domains like Finance / FinTech, indicating cross-domain applications.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"What are the judge scores for technically ambitious projects?\",\n",
        "        \"reference\": \"Technically ambitious projects generally receive high judge scores, often in the 9.0-9.5 range.\",\n",
        "        \"reference_contexts\": [\"Projects described as 'technically ambitious and well-executed' receive judge scores around 9.5.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"Are there projects combining security and finance?\",\n",
        "        \"reference\": \"Yes, some projects combine security as a primary domain with Finance / FinTech as a secondary domain.\",\n",
        "        \"reference_contexts\": [\"Project Aurora combines Security as primary domain with Finance / FinTech as secondary domain.\"]\n",
        "    },\n",
        "    {\n",
        "        \"user_input\": \"What is Project Aurora about?\",\n",
        "        \"reference\": \"Project Aurora (InsightAI 1) is a low-latency inference system for multimodal agents in autonomous systems, focused on Security with a Finance / FinTech secondary domain.\",\n",
        "        \"reference_contexts\": [\"Project Aurora focuses on a low-latency inference system for multimodal agents in autonomous systems, combining Security and Finance / FinTech domains.\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "golden_dataset_df = pd.DataFrame(golden_dataset)\n",
        "print(f\"âœ… Created golden dataset with {len(golden_dataset_df)} test questions\")\n",
        "print(\"\\nSample questions:\")\n",
        "for i, q in enumerate(golden_dataset_df['user_input'][:3], 1):\n",
        "    print(f\"  {i}. {q}\")\n",
        "    \n",
        "golden_dataset_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Prepare Retrievers for Evaluation\n",
        "\n",
        "Let's gather all our retrievers into a dictionary for systematic evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary of all retrievers to evaluate\n",
        "retrievers_to_evaluate = {\n",
        "    \"Naive (Vector Similarity)\": naive_retriever,\n",
        "    \"BM25\": bm25_retriever,\n",
        "    \"Contextual Compression (Rerank)\": compression_retriever,\n",
        "    \"Multi-Query\": multi_query_retriever,\n",
        "    \"Parent Document\": parent_document_retriever,\n",
        "    \"Ensemble\": ensemble_retriever\n",
        "}\n",
        "\n",
        "print(f\"Prepared {len(retrievers_to_evaluate)} retrievers for evaluation:\")\n",
        "for name in retrievers_to_evaluate.keys():\n",
        "    print(f\"  - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Evaluate Each Retriever with Ragas Metrics\n",
        "\n",
        "We'll use Ragas retriever-specific metrics:\n",
        "- **Context Precision**: How relevant are the retrieved contexts?\n",
        "- **Context Recall**: Did we retrieve all the necessary information?\n",
        "- **Context Entity Recall**: How well did we retrieve relevant entities?\n",
        "\n",
        "We'll also track cost and latency using LangSmith.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import ContextPrecision, ContextRecall, ContextEntityRecall\n",
        "from ragas import evaluate, EvaluationDataset, SingleTurnSample\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "import time\n",
        "from langsmith import Client\n",
        "\n",
        "# Initialize LangSmith client for tracking\n",
        "langsmith_client = Client()\n",
        "\n",
        "# Initialize models for Ragas evaluation\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "eval_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Initialize metrics with required models (Ragas 0.3.x API)\n",
        "context_precision = ContextPrecision(llm=eval_llm, embeddings=eval_embeddings)\n",
        "context_recall = ContextRecall(llm=eval_llm, embeddings=eval_embeddings)\n",
        "context_entity_recall = ContextEntityRecall(llm=eval_llm)\n",
        "\n",
        "# Function to retrieve contexts for each question\n",
        "def get_retriever_contexts(retriever, questions):\n",
        "    \"\"\"Retrieve contexts for a list of questions\"\"\"\n",
        "    contexts = []\n",
        "    for question in questions:\n",
        "        retrieved_docs = retriever.invoke(question)\n",
        "        # Extract page_content from each retrieved document\n",
        "        context_list = [doc.page_content for doc in retrieved_docs]\n",
        "        contexts.append(context_list)\n",
        "    return contexts\n",
        "\n",
        "print(\"Starting retriever evaluation...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for all retrievers\n",
        "evaluation_results = []\n",
        "\n",
        "# Extract questions and ground truths from golden dataset\n",
        "questions = golden_dataset_df['user_input'].tolist()\n",
        "ground_truths = golden_dataset_df['reference'].tolist()\n",
        "reference_contexts = golden_dataset_df['reference_contexts'].tolist()\n",
        "\n",
        "# Evaluate each retriever\n",
        "for retriever_name, retriever in retrievers_to_evaluate.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {retriever_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Get contexts using the retriever\n",
        "    try:\n",
        "        contexts = get_retriever_contexts(retriever, questions)\n",
        "        \n",
        "        retrieval_time = time.time() - start_time\n",
        "        avg_latency_ms = (retrieval_time / len(questions)) * 1000\n",
        "        \n",
        "        # Prepare dataset for Ragas evaluation (Ragas 0.3.x API)\n",
        "        # Create SingleTurnSample objects for each question\n",
        "        samples = []\n",
        "        for i, question in enumerate(questions):\n",
        "            sample = SingleTurnSample(\n",
        "                user_input=question,\n",
        "                retrieved_contexts=contexts[i],\n",
        "                reference_contexts=reference_contexts[i],\n",
        "                reference=ground_truths[i]\n",
        "            )\n",
        "            samples.append(sample)\n",
        "        \n",
        "        # Create EvaluationDataset\n",
        "        eval_dataset = EvaluationDataset(samples=samples)\n",
        "        \n",
        "        # Evaluate with Ragas metrics\n",
        "        print(f\"Running Ragas evaluation...\")\n",
        "        eval_start = time.time()\n",
        "        \n",
        "        result = evaluate(\n",
        "            dataset=eval_dataset,\n",
        "            metrics=[context_precision, context_recall, context_entity_recall]\n",
        "        )\n",
        "        \n",
        "        eval_time = time.time() - eval_start\n",
        "        \n",
        "        # Extract metric scores from the result\n",
        "        result_df = result.to_pandas()\n",
        "        \n",
        "        # Calculate mean scores for each metric\n",
        "        precision_score = result_df['context_precision'].mean() if 'context_precision' in result_df.columns else 0\n",
        "        recall_score = result_df['context_recall'].mean() if 'context_recall' in result_df.columns else 0\n",
        "        entity_recall_score = result_df['context_entity_recall'].mean() if 'context_entity_recall' in result_df.columns else 0\n",
        "        \n",
        "        # Store results\n",
        "        evaluation_results.append({\n",
        "            \"Retriever\": retriever_name,\n",
        "            \"Context Precision\": precision_score,\n",
        "            \"Context Recall\": recall_score,\n",
        "            \"Context Entity Recall\": entity_recall_score,\n",
        "            \"Avg Latency (ms)\": round(avg_latency_ms, 2),\n",
        "            \"Total Retrieval Time (s)\": round(retrieval_time, 2),\n",
        "            \"Evaluation Time (s)\": round(eval_time, 2)\n",
        "        })\n",
        "        \n",
        "        print(f\"âœ… Completed: {retriever_name}\")\n",
        "        print(f\"   - Context Precision: {precision_score:.4f}\")\n",
        "        print(f\"   - Context Recall: {recall_score:.4f}\")\n",
        "        print(f\"   - Context Entity Recall: {entity_recall_score:.4f}\")\n",
        "        print(f\"   - Avg Latency: {avg_latency_ms:.2f}ms\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error evaluating {retriever_name}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        evaluation_results.append({\n",
        "            \"Retriever\": retriever_name,\n",
        "            \"Context Precision\": 0,\n",
        "            \"Context Recall\": 0,\n",
        "            \"Context Entity Recall\": 0,\n",
        "            \"Avg Latency (ms)\": 0,\n",
        "            \"Total Retrieval Time (s)\": 0,\n",
        "            \"Evaluation Time (s)\": 0,\n",
        "            \"Error\": str(e)\n",
        "        })\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Evaluation complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Display Evaluation Results\n",
        "\n",
        "Let's visualize the results in a comprehensive table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# Sort by a composite score (weighted average of metrics)\n",
        "results_df['Composite Score'] = (\n",
        "    0.35 * results_df['Context Precision'] + \n",
        "    0.35 * results_df['Context Recall'] + \n",
        "    0.30 * results_df['Context Entity Recall']\n",
        ")\n",
        "\n",
        "# Sort by composite score\n",
        "results_df = results_df.sort_values('Composite Score', ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nðŸ“Š RETRIEVER EVALUATION RESULTS\")\n",
        "print(\"=\"*100)\n",
        "display(results_df)\n",
        "\n",
        "# Create a focused view\n",
        "print(\"\\nðŸŽ¯ Performance Ranking (by Composite Score):\")\n",
        "for idx, row in results_df.iterrows():\n",
        "    print(f\"{idx+1}. {row['Retriever']}: {row['Composite Score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Analyze Cost Information from LangSmith\n",
        "\n",
        "Let's fetch cost data from LangSmith to understand the financial implications of each retriever.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch cost information from LangSmith\n",
        "try:\n",
        "    # Get runs from the current project\n",
        "    runs = list(langsmith_client.list_runs(\n",
        "        project_name=\"09_Advanced_Retrieval_Evaluation\",\n",
        "        limit=500\n",
        "    ))\n",
        "    \n",
        "    if runs:\n",
        "        total_cost = sum([run.total_cost for run in runs if run.total_cost])\n",
        "        total_tokens = sum([run.total_tokens for run in runs if run.total_tokens])\n",
        "        \n",
        "        print(f\"\\nðŸ’° COST ANALYSIS FROM LANGSMITH\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total Runs Tracked: {len(runs)}\")\n",
        "        print(f\"Total Cost: ${total_cost:.4f}\")\n",
        "        print(f\"Total Tokens Used: {total_tokens:,}\")\n",
        "        print(f\"Average Cost per Run: ${total_cost/len(runs):.6f}\")\n",
        "        \n",
        "        # Estimate cost per retriever (approximate)\n",
        "        est_cost_per_retriever = total_cost / len(retrievers_to_evaluate)\n",
        "        print(f\"\\nEstimated Cost per Retriever: ${est_cost_per_retriever:.4f}\")\n",
        "        \n",
        "        # Add cost estimates to results\n",
        "        results_df['Estimated Cost ($)'] = est_cost_per_retriever\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸ No runs found in LangSmith. Make sure tracing is enabled.\")\n",
        "        print(\"You can view detailed cost and latency information in the LangSmith UI:\")\n",
        "        print(\"https://smith.langchain.com/\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not fetch cost data from LangSmith: {e}\")\n",
        "    print(\"Note: You can view detailed cost and latency information in the LangSmith UI:\")\n",
        "    print(\"https://smith.langchain.com/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Visualize the Results\n",
        "\n",
        "Let's create visualizations to better understand the trade-offs between retrievers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create subplots for different metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Retriever Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Performance Metrics Comparison\n",
        "ax1 = axes[0, 0]\n",
        "metrics = ['Context Precision', 'Context Recall', 'Context Entity Recall']\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.25\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax1.bar(x + i*width, results_df[metric], width, label=metric, alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Retriever')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Performance Metrics by Retriever')\n",
        "ax1.set_xticks(x + width)\n",
        "ax1.set_xticklabels(results_df['Retriever'], rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Composite Score\n",
        "ax2 = axes[0, 1]\n",
        "bars = ax2.barh(results_df['Retriever'], results_df['Composite Score'], color='skyblue')\n",
        "ax2.set_xlabel('Composite Score')\n",
        "ax2.set_title('Overall Performance Ranking')\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Color the best performer\n",
        "bars[0].set_color('gold')\n",
        "\n",
        "# 3. Latency Comparison\n",
        "ax3 = axes[1, 0]\n",
        "ax3.bar(results_df['Retriever'], results_df['Avg Latency (ms)'], color='coral', alpha=0.7)\n",
        "ax3.set_xlabel('Retriever')\n",
        "ax3.set_ylabel('Average Latency (ms)')\n",
        "ax3.set_title('Latency Comparison')\n",
        "ax3.set_xticklabels(results_df['Retriever'], rotation=45, ha='right')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Performance vs Latency Trade-off\n",
        "ax4 = axes[1, 1]\n",
        "scatter = ax4.scatter(results_df['Avg Latency (ms)'], \n",
        "                      results_df['Composite Score'], \n",
        "                      s=200, \n",
        "                      c=results_df['Composite Score'], \n",
        "                      cmap='viridis',\n",
        "                      alpha=0.6,\n",
        "                      edgecolors='black',\n",
        "                      linewidth=2)\n",
        "\n",
        "# Annotate points\n",
        "for idx, row in results_df.iterrows():\n",
        "    ax4.annotate(row['Retriever'], \n",
        "                (row['Avg Latency (ms)'], row['Composite Score']),\n",
        "                fontsize=8,\n",
        "                ha='right')\n",
        "\n",
        "ax4.set_xlabel('Average Latency (ms)')\n",
        "ax4.set_ylabel('Composite Score')\n",
        "ax4.set_title('Performance vs Latency Trade-off')\n",
        "ax4.grid(alpha=0.3)\n",
        "plt.colorbar(scatter, ax=ax4, label='Composite Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Final Analysis and Recommendations\n",
        "\n",
        "Based on the evaluation results, let's provide a comprehensive analysis considering performance, cost, and latency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"ðŸ“ COMPREHENSIVE ANALYSIS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "# Get the best performers in different categories\n",
        "best_overall = results_df.iloc[0]\n",
        "fastest = results_df.loc[results_df['Avg Latency (ms)'].idxmin()]\n",
        "best_precision = results_df.loc[results_df['Context Precision'].idxmax()]\n",
        "best_recall = results_df.loc[results_df['Context Recall'].idxmax()]\n",
        "\n",
        "analysis = f\"\"\"\n",
        "EXECUTIVE SUMMARY:\n",
        "\n",
        "After evaluating {len(retrievers_to_evaluate)} different retrieval methods on our synthetic usecase dataset, \n",
        "here are the key findings:\n",
        "\n",
        "ðŸ† BEST OVERALL PERFORMER: {best_overall['Retriever']}\n",
        "   - Composite Score: {best_overall['Composite Score']:.4f}\n",
        "   - Context Precision: {best_overall['Context Precision']:.4f}\n",
        "   - Context Recall: {best_overall['Context Recall']:.4f}\n",
        "   - Context Entity Recall: {best_overall['Context Entity Recall']:.4f}\n",
        "   - Average Latency: {best_overall['Avg Latency (ms)']:.2f}ms\n",
        "\n",
        "âš¡ FASTEST RETRIEVER: {fastest['Retriever']}\n",
        "   - Average Latency: {fastest['Avg Latency (ms)']:.2f}ms\n",
        "   - Composite Score: {fastest['Composite Score']:.4f}\n",
        "\n",
        "ðŸŽ¯ BEST PRECISION: {best_precision['Retriever']}\n",
        "   - Context Precision: {best_precision['Context Precision']:.4f}\n",
        "\n",
        "ðŸ“š BEST RECALL: {best_recall['Retriever']}\n",
        "   - Context Recall: {best_recall['Context Recall']:.4f}\n",
        "\n",
        "DETAILED ANALYSIS:\n",
        "\n",
        "For this particular synthetic usecase dataset about AI projects with various domains, the evaluation \n",
        "reveals important trade-offs between different retrieval strategies:\n",
        "\n",
        "1. PERFORMANCE ANALYSIS:\n",
        "   {best_overall['Retriever']} emerged as the top performer with a composite score of {best_overall['Composite Score']:.4f}.\n",
        "   This retriever demonstrates {\n",
        "       'strong' if best_overall['Context Precision'] > 0.7 else \n",
        "       'moderate' if best_overall['Context Precision'] > 0.5 else 'limited'\n",
        "   } precision ({best_overall['Context Precision']:.4f}) and {\n",
        "       'excellent' if best_overall['Context Recall'] > 0.7 else \n",
        "       'good' if best_overall['Context Recall'] > 0.5 else 'adequate'\n",
        "   } recall ({best_overall['Context Recall']:.4f}), making it highly effective at retrieving \n",
        "   relevant project information while minimizing irrelevant results.\n",
        "\n",
        "2. LATENCY CONSIDERATIONS:\n",
        "   The fastest retriever, {fastest['Retriever']}, processes queries in just {fastest['Avg Latency (ms)']:.2f}ms\n",
        "   compared to {results_df['Avg Latency (ms)'].max():.2f}ms for the slowest. For production systems\n",
        "   requiring real-time responses, {'this makes it an excellent choice despite potentially lower accuracy'\n",
        "   if fastest['Retriever'] != best_overall['Retriever'] else 'its combination of speed and accuracy is ideal'}.\n",
        "\n",
        "3. COST EFFICIENCY:\n",
        "   Based on LangSmith tracking, retrieval operations have measurable costs through API calls to \n",
        "   embedding models and LLMs (for multi-query and evaluation). Methods like BM25 that rely on \n",
        "   lexical matching have zero ongoing API costs, while reranking and multi-query approaches \n",
        "   incur additional expenses through multiple model calls.\n",
        "\n",
        "4. USE CASE RECOMMENDATIONS:\n",
        "   \n",
        "   - For PRODUCTION SYSTEMS with strict latency requirements (<100ms): \n",
        "     Use {fastest['Retriever']} or implement caching strategies\n",
        "   \n",
        "   - For ACCURACY-CRITICAL APPLICATIONS (research, compliance):\n",
        "     Use {best_overall['Retriever']} to maximize retrieval quality\n",
        "   \n",
        "   - For COST-SENSITIVE DEPLOYMENTS:\n",
        "     Start with BM25 or Naive retrieval, then add complexity only if needed\n",
        "   \n",
        "   - For BALANCED APPROACH:\n",
        "     Ensemble methods can provide good performance by combining multiple strategies,\n",
        "     though they increase latency and cost proportionally\n",
        "\n",
        "CONCLUSION:\n",
        "{\n",
        "f'''The {best_overall['Retriever']} retriever is recommended for this dataset because it achieves \n",
        "the best balance of precision and recall, ensuring that users receive comprehensive and relevant \n",
        "project information. While it may have {'higher latency than simpler methods' if best_overall['Avg Latency (ms)'] > fastest['Avg Latency (ms)'] else 'excellent latency characteristics'}, \n",
        "the improved accuracy justifies the tradeoff for most use cases. Organizations should consider \n",
        "implementing caching, async processing, or tiered retrieval strategies to optimize the cost-performance-latency \n",
        "triangle based on their specific requirements.'''\n",
        "}\n",
        "\n",
        "For detailed cost breakdowns and trace-level latency analysis, refer to the LangSmith dashboard at:\n",
        "https://smith.langchain.com/projects/09_Advanced_Retrieval_Evaluation\n",
        "\"\"\"\n",
        "\n",
        "print(analysis)\n",
        "\n",
        "# Save results to CSV for further analysis\n",
        "results_df.to_csv('retriever_evaluation_results.csv', index=False)\n",
        "print(\"\\nâœ… Results saved to 'retriever_evaluation_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Key Takeaways\n",
        "\n",
        "**Summary of Retriever Evaluation:**\n",
        "\n",
        "This comprehensive evaluation demonstrates that choosing the right retrieval strategy depends heavily on your specific requirements:\n",
        "\n",
        "1. **No One-Size-Fits-All Solution**: Each retriever has distinct strengths and weaknesses across performance, cost, and latency dimensions.\n",
        "\n",
        "2. **The Performance-Latency Trade-off**: More sophisticated retrievers (Multi-Query, Contextual Compression, Ensemble) typically achieve better accuracy but at the cost of increased latency and API calls.\n",
        "\n",
        "3. **Ensemble Benefits**: Ensemble retrievers can leverage the strengths of multiple approaches, though they compound latency and costs.\n",
        "\n",
        "4. **Cost Considerations**: \n",
        "   - BM25: Zero ongoing API costs (pure keyword matching)\n",
        "   - Naive/Parent Document: Minimal costs (single embedding call)\n",
        "   - Multi-Query: 3-5x embedding costs (multiple queries)\n",
        "   - Contextual Compression: Additional reranking API costs\n",
        "   - Ensemble: Combined costs of all constituent retrievers\n",
        "\n",
        "5. **Production Recommendations**:\n",
        "   - **Start simple**: Begin with Naive or BM25, measure baseline performance\n",
        "   - **Iterate intelligently**: Add complexity only where needed based on actual performance gaps\n",
        "   - **Monitor continuously**: Use LangSmith to track costs and latency in production\n",
        "   - **Consider caching**: Cache frequent queries to amortize retrieval costs\n",
        "   - **Hybrid approaches**: Combine cheap BM25 for initial filtering with expensive reranking for top candidates\n",
        "\n",
        "6. **Dataset-Specific Insights**: \n",
        "   For structured project data with clear domains and descriptions, the retrieval strategy should align with query patterns:\n",
        "   - Exact term matches â†’ BM25\n",
        "   - Semantic similarity â†’ Vector embeddings\n",
        "   - Complex queries â†’ Multi-Query or Ensemble\n",
        "   - Maximum accuracy â†’ Contextual Compression (Rerank)\n",
        "\n",
        "**Next Steps**: \n",
        "- Review the LangSmith dashboard for detailed trace analysis\n",
        "- Experiment with different retriever configurations\n",
        "- Test with real user queries to validate synthetic evaluation results\n",
        "- Implement A/B testing in production to measure real-world impact\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (Advanced Retrieval)",
      "language": "python",
      "name": "advanced-retrieval"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
