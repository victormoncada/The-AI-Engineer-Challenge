{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- ü§ù Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "- ü§ù Breakout Room #2:\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQ3nRAgoF67"
   },
   "source": [
    "# ü§ù Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set our OpenAI, Tavily, and LangSmith API keys along with our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jkla2fpx28QK",
    "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LangSmith API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE8 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "#### üèóÔ∏è Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "tool_belt = [\n",
    "    tavily_tool,\n",
    "    ArxivQueryRun(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "model = model.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERzuGo6W18Lr"
   },
   "source": [
    "#### ‚ùì Question #1:\n",
    "\n",
    "How does the model determine which tool to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `tool_node` is a node which can call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vF4_lgtmQNo",
    "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110d94e10>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGCbaYqRnmiw",
    "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110d94e10>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BZgb81VQf9o",
    "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110d94e10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  return END\n",
    "\n",
    "uncompiled_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvcgbHf1rIXZ",
    "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110d94e10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "simple_agent_graph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhNWIwBL1W4Q"
   },
   "source": [
    "#### ‚ùì Question #2:\n",
    "\n",
    "Is there any specific limit to how many times we can cycle?\n",
    "\n",
    "If not, how could we impose a limit to the number of cycles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='AI is increasingly embedded across technical professions, acting as an assistant that automates routine work, augments decision-making with data-driven insights, and enables new design and optimization capabilities. Here‚Äôs a practical breakdown by area, plus how to start.\\n\\nKey ways technical professionals use AI\\n\\n- Software engineering and IT\\n  - Code assistance: AI copilots (e.g., GitHub Copilot, CodeWhisperer, Tabnine) help with autocomplete, boilerplate, refactoring suggestions, and even generating tests.\\n  - Documentation and onboarding: automatic API doc generation, README drafting, and natural-language summaries of codebases.\\n  - Testing and quality: automated test case generation, property-based testing suggestions, smart mutation testing, and AI-guided fuzzing.\\n  - DevOps and reliability: anomaly detection on metrics, automated incident response playbooks, predictive capacity planning, log analysis, and self-optimizing CI/CD pipelines.\\n  - Security: pattern-based and anomaly detection for threats, faster triage of alerts, and vulnerability priors.\\n\\n- Data science, analytics, and data engineering\\n  - Data wrangling and feature engineering: automatic schema discovery, cleaning suggestions, and feature suggestions.\\n  - Model selection and tuning: automated ML (AutoML), hyperparameter optimization, model ensembling guidance.\\n  - Experiment tracking and governance: model monitoring for drift, bias checks, and explainability dashboards.\\n  - Operationalization: AI-assisted data quality checks, data lineage, and automated retraining pipelines.\\n\\n- AI-assisted research, product, and engineering design\\n  - Generative design: topology optimization and parametric design in CAD tools, creating multiple feasible concepts within constraints.\\n  - Digital twins and simulations: surrogate models to accelerate multi-physics simulations and real-time decision support.\\n  - Documentation and literature review: AI-assisted summaries, citations mapping, and synthesis of findings.\\n\\n- Hardware, electronics, and manufacturing\\n  - AI in electronic design automation (EDA): ML-based placement/routing, timing prediction, and defect detection in fabrication.\\n  - Predictive maintenance: ML models predict failure in machines and sensors, reducing downtime.\\n  - Quality inspection and process control: computer vision for defect detection on assembly lines.\\n\\n- Cybersecurity and networking\\n  - Threat detection: ML-based anomaly detection in logs and network traffic.\\n  - Security operations support: automated triage, incident response playbooks, and risk scoring.\\n  - Network optimization: traffic forecasting and demand-aware resource allocation.\\n\\n- Civil, mechanical, aerospace, and energy engineering\\n  - Structural optimization: generative/AI-driven optimization of materials and geometries.\\n  - Performance modeling: surrogate models to speed up design iterations and multi-disciplinary optimization.\\n  - Maintenance and reliability: prognostics for equipment health and energy systems.\\n\\n- Operations, supply chain, and manufacturing\\n  - Demand forecasting and optimization: ML-driven demand signals and inventory optimization.\\n  - Robotics and automation: AI planning for warehouse robots and autonomous systems.\\n  - Quality and defect detection: AI vision systems for real-time quality inspection.\\n\\nPractical tips to get started\\n\\n- Start small and measurable\\n  - Pick a pain point: repetitive data cleaning, code compilation times, test generation, or incident triage.\\n  - Define a concrete metric: time saved, defect rate reduction, or faster incident response.\\n\\n- Build the right foundations\\n  - Data quality and governance: ensure clean data, reproducible pipelines, and clear provenance.\\n  - Human-in-the-loop: keep expert oversight for critical decisions; AI as a partner, not a black box.\\n  - Reproducibility: version data, models, and code; use standard ML/Ops tooling where possible.\\n\\n- Choose the right tools for the job\\n  - Software development: code completion, documentation, test generation, and CI/CD optimization.\\n  - Data work: AutoML, feature stores, model monitoring, drift detection.\\n  - Design/engineering: generative design tools in CAD/CA D, surrogate models for simulations.\\n  - Ops/SRE: anomaly detection dashboards, AI-assisted alert triage, automated runbooks.\\n\\n- Manage risk and ethics\\n  - Evaluate model performance in production, monitor for drift, and implement fallback plans.\\n  - Address bias, data privacy, and security implications; ensure explainability where needed.\\n  - Keep humans in the loop for decisions with safety or regulatory impact.\\n\\n- Example starter projects by role\\n  - Software engineer: enable AI-assisted code completion for a microservice; measure development time saved.\\n  - Data scientist: replace one manual feature engineering step with an automated feature tool and track improvement in model quality.\\n  - DevOps engineer: set up AI-based anomaly detection on production metrics and create automated incident runbooks.\\n  - Mechanical/industrial designer: run a pilot generative design task for a component with constraints; compare performance and manufacturability to the current design.\\n\\nWhat‚Äôs changing on the horizon\\n\\n- More mainstream AI copilots embedded across toolchains, making AI-enabled work more ubiquitous.\\n- Edge and embedded AI enabling intelligent devices and offline capabilities.\\n- Better MLOps practices, model governance, and safety controls to scale AI responsibly.\\n- Increased focus on data-centric AI: quality data and robust data pipelines as the biggest lever for performance.\\n\\nWould you like a tailored overview for a specific field, job role, or company size? If you share your domain and current challenges, I can suggest concrete pilot ideas, tools, and success metrics.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2388, 'prompt_tokens': 244, 'total_tokens': 2632, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1280, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CJ7sioC4wwZfE3Lvyb3PJd4ZT9hrD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e7d9c4c-646d-479b-9d77-0549b149e7c0-0', usage_metadata={'input_tokens': 244, 'output_tokens': 2388, 'total_tokens': 2632, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1280}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UmMzZUABTKj7L7oBssXC1JEe', 'function': {'arguments': '{\"query\":\"A Comprehensive Survey of Deep Research\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 540, 'prompt_tokens': 263, 'total_tokens': 803, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CJ7swjHTNzODlD0swEiKqEfi4GJZF', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e659886e-5657-4994-8637-abdf8f0c8116-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'A Comprehensive Survey of Deep Research'}, 'id': 'call_UmMzZUABTKj7L7oBssXC1JEe', 'type': 'tool_call'}], usage_metadata={'input_tokens': 263, 'output_tokens': 540, 'total_tokens': 803, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: arxiv\n",
      "[ToolMessage(content='Published: 2025-06-14\\nTitle: A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\nAuthors: Renjun Xu, Jingwen Peng\\nSummary: This survey examines the rapidly evolving field of Deep Research systems --\\nAI-powered applications that automate complex research workflows through the\\nintegration of large language models, advanced information retrieval, and\\nautonomous reasoning capabilities. We analyze more than 80 commercial and\\nnon-commercial implementations that have emerged since 2023, including\\nOpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and\\nnumerous open-source alternatives. Through comprehensive examination, we\\npropose a novel hierarchical taxonomy that categorizes systems according to\\nfour fundamental technical dimensions: foundation models and reasoning engines,\\ntool utilization and environmental interaction, task planning and execution\\ncontrol, and knowledge synthesis and output generation. We explore the\\narchitectural patterns, implementation approaches, and domain-specific\\nadaptations that characterize these systems across academic, scientific,\\nbusiness, and educational applications. Our analysis reveals both the\\nsignificant capabilities of current implementations and the technical and\\nethical challenges they present regarding information accuracy, privacy,\\nintellectual property, and accessibility. The survey concludes by identifying\\npromising research directions in advanced reasoning architectures, multimodal\\nintegration, domain specialization, human-AI collaboration, and ecosystem\\nstandardization that will likely shape the future evolution of this\\ntransformative technology. By providing a comprehensive framework for\\nunderstanding Deep Research systems, this survey contributes to both the\\ntheoretical understanding of AI-augmented knowledge work and the practical\\ndevelopment of more capable, responsible, and accessible research technologies.\\nThe paper resources can be viewed at\\nhttps://github.com/scienceaix/deepresearch.\\n\\nPublished: 2021-03-05\\nTitle: A comprehensive survey on point cloud registration\\nAuthors: Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas\\nSummary: Registration is a transformation estimation problem between two point clouds,\\nwhich has a unique and critical role in numerous computer vision applications.\\nThe developments of optimization-based methods and deep learning methods have\\nimproved registration robustness and efficiency. Recently, the combinations of\\noptimization-based and deep learning methods have further improved performance.\\nHowever, the connections between optimization-based and deep learning methods\\nare still unclear. Moreover, with the recent development of 3D sensors and 3D\\nreconstruction techniques, a new research direction emerges to align\\ncross-source point clouds. This survey conducts a comprehensive survey,\\nincluding both same-source and cross-source registration methods, and summarize\\nthe connections between optimization-based and deep learning methods, to\\nprovide further research insight. This survey also builds a new benchmark to\\nevaluate the state-of-the-art registration algorithms in solving cross-source\\nchallenges. Besides, this survey summarizes the benchmark data sets and\\ndiscusses point cloud registration applications across various domains.\\nFinally, this survey proposes potential research directions in this rapidly\\ngrowing field.\\n\\nPublished: 2023-07-07\\nTitle: A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision\\nAuthors: Zhonghan Zhao, Wenhao Chai, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Mingli Song, Jenq-Neng Hwang, Gaoang Wang\\nSummary: Deep learning has the potential to revolutionize sports performance, with\\napplications ranging from perception and comprehension to decision. This paper\\npresents a comprehensive survey of deep learning in sports performance,\\nfocusing on three main aspects: algorithms, datasets and virtual environments,\\nand challenges. Firstly, we discuss th', name='arxiv', id='45b4bf62-c300-41d0-b31d-23ad09f68c8c', tool_call_id='call_UmMzZUABTKj7L7oBssXC1JEe')]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vWGZ8M3kWqvTOvSVc71bJysF', 'function': {'arguments': '{\"query\": \"Renjun Xu current affiliation\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_pb1wdN61FYfLacCinEkwTNBU', 'function': {'arguments': '{\"query\": \"Jingwen Peng current affiliation\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 774, 'prompt_tokens': 1037, 'total_tokens': 1811, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CJ7t1RHKE2XYLdq9DfApIDGH8MAy1', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d916bf60-b508-4c41-a879-48b5d6687b50-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Renjun Xu current affiliation'}, 'id': 'call_vWGZ8M3kWqvTOvSVc71bJysF', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Jingwen Peng current affiliation'}, 'id': 'call_pb1wdN61FYfLacCinEkwTNBU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1037, 'output_tokens': 774, 'total_tokens': 1811, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: tavily_search_results_json\n",
      "[ToolMessage(content='[{\"title\": \"Renjun Xu - Researcher, Zhejiang University - OpenReview\", \"url\": \"https://openreview.net/profile?id=~Renjun_Xu1\", \"content\": \"# Renjun Xu\\\\n\\\\n### Principal Researcher, Zhejiang University\\\\n\\\\n#### Names\\\\n\\\\n#### Emails\\\\n\\\\n#### Personal Links\\\\n\\\\n#### Career & Education History\\\\n\\\\n#### Advisors, Relations & Conflicts\\\\n\\\\nNo relations added\\\\n\\\\n#### Expertise\\\\n\\\\n#### Publications\\\\n\\\\n#### scKGOT: Intercellular Signaling Inference with Knowledge Graph Optimal Transport for Single-cell Transcriptomics)\\\\n\\\\n#### $E(2)$-Equivariant Vision Transformer)\\\\n\\\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review) [...] #### Learning Universal User Representations via Self-Supervised Lifelong Behaviors Modeling)\\\\n\\\\n#### Learning Invariant Representations across Domains and Tasks)\\\\n\\\\n#### Co-Authors\\\\n\\\\nOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ¬© 2025 OpenReview [...] #### Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition)\\\\n\\\\n#### Hierarchical knowledge amalgamation with dual discriminative feature alignment)\\\\n\\\\n#### Modeling Dynamic Missingness of Implicit Feedback for Sequential Recommendation)\\\\n\\\\n#### S2SNet: A Pretrained Neural Network for Superconductivity Discovery)\\\\n\\\\n#### Learning Interest-oriented Universal User Representation via Self-supervision)\", \"score\": 0.529045}, {\"title\": \"Renjun Xu\", \"url\": \"https://aclanthology.org/people/renjun-xu/\", \"content\": \"ACL Logo\\\\n\\\\n## Renjun Xu\\\\n\\\\n#### 2025\\\\n\\\\npdfbibabs  \\\\nNOVA: An Iterative Planning Framework for Enhancing Scientific Innovation with Large Language Models  \\\\nXiang Hu\\\\n|\\\\nHongyu Fu\\\\n|\\\\nJinge Wang\\\\n|\\\\nYifeng Wang\\\\n|\\\\nZhikun Li\\\\n|\\\\nRenjun Xu\\\\n|\\\\nYu Lu\\\\n|\\\\nYaochu Jin\\\\n|\\\\nLili Pan\\\\n|\\\\nZhenzhong Lan  \\\\nFindings of the Association for Computational Linguistics: ACL 2025\\\\n\\\\n##### Co-authors\\\\n\\\\n##### Venues [...] Creative Commons License\\\\nACL materials are Copyright ¬© 1963‚Äì2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\\\\n\\\\nCreative Commons License [...] The ACL Anthology is managed and built by the ACL Anthology team of volunteers.\\\\n\\\\nSite last built on 21 September 2025 at 18:09 UTC with commit 81c241e.\", \"score\": 0.33047172}, {\"title\": \"Do Large Language Models Truly Grasp Addition? A Rule- ...\", \"url\": \"https://arxiv.org/html/2504.05262v3\", \"content\": \"Do Large Language Models Truly Grasp Addition?\\\\n  \\\\nA Rule-Focused Diagnostic Using Two-Integer Arithmetic\\\\n\\\\nYang Yan1,2,\\\\nYu Lu2,\\\\nRenjun Xu1,‚Ä†‚Ä†thanks: Corresponding Authors.,\\\\nZhenzhong Lan2,11footnotemark: 1\\\\n\\\\n1 Zhejiang University\\\\n\\\\n2 School of Engineering, Westlake University\\\\n\\\\n{yan.yang,rux}@zju.edu.cn\\\\n\\\\n{yanyang,luyu,lanzhenzhong}@westlake.edu.cn\\\\n\\\\nRefer to caption\\\\n\\\\n## 1 Introduction\\\\n\\\\n##### Why basic arithmetic still matters. [...] Mechanistic studies of arithmetic circuits.\\\\nNeuron-level inspections report units that store partial carries, as well as heuristics that fail outside the training range (Qiu et al., 2024; Nikankin et al., 2025).\\\\nGrokking phenomena illustrate that models can memorize before they generalize, and sometimes never reach full rule induction (Power et al., 2022). [...] ##### Contributions.\\\\n\\\\nOur main contributions are:\\\\n\\\\nDiagnostic Methodology: We introduce a diagnostic methodology centered on two-integer addition, using notation invariance, digit-scaling consistency, and algebraic integrity as key criteria to differentiate genuine rule learning from superficial pattern matching in LLMs.\", \"score\": 0.20004182}, {\"title\": \"Time-resolved T1 and T2 contrast for enhanced accuracy in MRI ...\", \"url\": \"https://pubmed.ncbi.nlm.nih.gov/40187097/\", \"content\": \"### Add to Collections\\\\n\\\\n### Add to My Bibliography\\\\n\\\\n## Your saved search\\\\n\\\\n## Create a file for external citation management software\\\\n\\\\n## Your RSS Feed\\\\n\\\\n### Full text links\\\\n\\\\nElsevier Science full text link\\\\n\\\\n### Actions\\\\n\\\\n### Page navigation\\\\n\\\\n# Time-resolved T1 and T2 contrast for enhanced accuracy in MRI tumor detection\\\\n\\\\n### Affiliations\\\\n\\\\n# Time-resolved T1 and T2 contrast for enhanced accuracy in MRI tumor detection\\\\n\\\\n### Authors\\\\n\\\\n### Affiliations\\\\n\\\\n## Abstract [...] ### Full Text Sources\\\\n\\\\n### Other Literature Sources\\\\n\\\\n### Medical\\\\n\\\\nElsevier Science full text link\\\\n\\\\nNCBI Literature Resources\\\\n\\\\nMeSH\\\\nPMC\\\\nBookshelf\\\\nDisclaimer\\\\n\\\\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.\\\\n\\\\nConnect with NLM\\\\n\\\\nNational Library of Medicine  \\\\n8600 Rockville Pike  \\\\nBethesda, MD 20894\\\\n\\\\nWeb Policies  \\\\nFOIA  \\\\nHHS Vulnerability Disclosure [...] Keywords:\\\\nDual responsiveness; Sequential T(1) and T(2) contrast; Smart contrast agent; Tumor diagnosis; Ultra-high accuracy.\\\\n\\\\nCopyright ¬© 2025 Elsevier Ltd. All rights reserved.\\\\n\\\\nPubMed Disclaimer\\\\n\\\\n## Conflict of interest statement\\\\n\\\\nDeclaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\\\\n\\\\n## MeSH terms\\\\n\\\\n## Substances\\\\n\\\\n## LinkOut - more resources\", \"score\": 0.046625108}, {\"title\": \"Juntao Jiang - OpenReview\", \"url\": \"https://openreview.net/profile?id=~Juntao_Jiang1\", \"content\": \"# Juntao Jiang\\\\n\\\\n### PhD student, College of Control Science and Engineering, Zhejiang University\\\\n\\\\n#### Names\\\\n\\\\n#### Emails\\\\n\\\\n#### Personal Links\\\\n\\\\n#### Career & Education History\\\\n\\\\n#### Advisors, Relations & Conflicts\\\\n\\\\n#### Expertise\\\\n\\\\n#### Publications\\\\n\\\\n#### Can Frozen Transformers in Large Language Models Help with Medical Image Segmentation?)\\\\n\\\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review)\", \"score\": 0.025782676}]', name='tavily_search_results_json', id='89d7341e-654e-48c9-9eea-0ff31aef6dbb', tool_call_id='call_vWGZ8M3kWqvTOvSVc71bJysF', artifact={'query': 'Renjun Xu current affiliation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://openreview.net/profile?id=~Renjun_Xu1', 'title': 'Renjun Xu - Researcher, Zhejiang University - OpenReview', 'content': '# Renjun Xu\\n\\n### Principal Researcher, Zhejiang University\\n\\n#### Names\\n\\n#### Emails\\n\\n#### Personal Links\\n\\n#### Career & Education History\\n\\n#### Advisors, Relations & Conflicts\\n\\nNo relations added\\n\\n#### Expertise\\n\\n#### Publications\\n\\n#### scKGOT: Intercellular Signaling Inference with Knowledge Graph Optimal Transport for Single-cell Transcriptomics)\\n\\n#### $E(2)$-Equivariant Vision Transformer)\\n\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review) [...] #### Learning Universal User Representations via Self-Supervised Lifelong Behaviors Modeling)\\n\\n#### Learning Invariant Representations across Domains and Tasks)\\n\\n#### Co-Authors\\n\\nOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ¬© 2025 OpenReview [...] #### Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition)\\n\\n#### Hierarchical knowledge amalgamation with dual discriminative feature alignment)\\n\\n#### Modeling Dynamic Missingness of Implicit Feedback for Sequential Recommendation)\\n\\n#### S2SNet: A Pretrained Neural Network for Superconductivity Discovery)\\n\\n#### Learning Interest-oriented Universal User Representation via Self-supervision)', 'score': 0.529045, 'raw_content': None}, {'url': 'https://aclanthology.org/people/renjun-xu/', 'title': 'Renjun Xu', 'content': 'ACL Logo\\n\\n## Renjun Xu\\n\\n#### 2025\\n\\npdfbibabs  \\nNOVA: An Iterative Planning Framework for Enhancing Scientific Innovation with Large Language Models  \\nXiang Hu\\n|\\nHongyu Fu\\n|\\nJinge Wang\\n|\\nYifeng Wang\\n|\\nZhikun Li\\n|\\nRenjun Xu\\n|\\nYu Lu\\n|\\nYaochu Jin\\n|\\nLili Pan\\n|\\nZhenzhong Lan  \\nFindings of the Association for Computational Linguistics: ACL 2025\\n\\n##### Co-authors\\n\\n##### Venues [...] Creative Commons License\\nACL materials are Copyright ¬© 1963‚Äì2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\\n\\nCreative Commons License [...] The ACL Anthology is managed and built by the ACL Anthology team of volunteers.\\n\\nSite last built on 21 September 2025 at 18:09 UTC with commit 81c241e.', 'score': 0.33047172, 'raw_content': None}, {'url': 'https://arxiv.org/html/2504.05262v3', 'title': 'Do Large Language Models Truly Grasp Addition? A Rule- ...', 'content': 'Do Large Language Models Truly Grasp Addition?\\n  \\nA Rule-Focused Diagnostic Using Two-Integer Arithmetic\\n\\nYang Yan1,2,\\nYu Lu2,\\nRenjun Xu1,‚Ä†‚Ä†thanks: Corresponding Authors.,\\nZhenzhong Lan2,11footnotemark: 1\\n\\n1 Zhejiang University\\n\\n2 School of Engineering, Westlake University\\n\\n{yan.yang,rux}@zju.edu.cn\\n\\n{yanyang,luyu,lanzhenzhong}@westlake.edu.cn\\n\\nRefer to caption\\n\\n## 1 Introduction\\n\\n##### Why basic arithmetic still matters. [...] Mechanistic studies of arithmetic circuits.\\nNeuron-level inspections report units that store partial carries, as well as heuristics that fail outside the training range (Qiu et al., 2024; Nikankin et al., 2025).\\nGrokking phenomena illustrate that models can memorize before they generalize, and sometimes never reach full rule induction (Power et al., 2022). [...] ##### Contributions.\\n\\nOur main contributions are:\\n\\nDiagnostic Methodology: We introduce a diagnostic methodology centered on two-integer addition, using notation invariance, digit-scaling consistency, and algebraic integrity as key criteria to differentiate genuine rule learning from superficial pattern matching in LLMs.', 'score': 0.20004182, 'raw_content': None}, {'url': 'https://pubmed.ncbi.nlm.nih.gov/40187097/', 'title': 'Time-resolved T1 and T2 contrast for enhanced accuracy in MRI ...', 'content': '### Add to Collections\\n\\n### Add to My Bibliography\\n\\n## Your saved search\\n\\n## Create a file for external citation management software\\n\\n## Your RSS Feed\\n\\n### Full text links\\n\\nElsevier Science full text link\\n\\n### Actions\\n\\n### Page navigation\\n\\n# Time-resolved T1 and T2 contrast for enhanced accuracy in MRI tumor detection\\n\\n### Affiliations\\n\\n# Time-resolved T1 and T2 contrast for enhanced accuracy in MRI tumor detection\\n\\n### Authors\\n\\n### Affiliations\\n\\n## Abstract [...] ### Full Text Sources\\n\\n### Other Literature Sources\\n\\n### Medical\\n\\nElsevier Science full text link\\n\\nNCBI Literature Resources\\n\\nMeSH\\nPMC\\nBookshelf\\nDisclaimer\\n\\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.\\n\\nConnect with NLM\\n\\nNational Library of Medicine  \\n8600 Rockville Pike  \\nBethesda, MD 20894\\n\\nWeb Policies  \\nFOIA  \\nHHS Vulnerability Disclosure [...] Keywords:\\nDual responsiveness; Sequential T(1) and T(2) contrast; Smart contrast agent; Tumor diagnosis; Ultra-high accuracy.\\n\\nCopyright ¬© 2025 Elsevier Ltd. All rights reserved.\\n\\nPubMed Disclaimer\\n\\n## Conflict of interest statement\\n\\nDeclaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\\n\\n## MeSH terms\\n\\n## Substances\\n\\n## LinkOut - more resources', 'score': 0.046625108, 'raw_content': None}, {'url': 'https://openreview.net/profile?id=~Juntao_Jiang1', 'title': 'Juntao Jiang - OpenReview', 'content': '# Juntao Jiang\\n\\n### PhD student, College of Control Science and Engineering, Zhejiang University\\n\\n#### Names\\n\\n#### Emails\\n\\n#### Personal Links\\n\\n#### Career & Education History\\n\\n#### Advisors, Relations & Conflicts\\n\\n#### Expertise\\n\\n#### Publications\\n\\n#### Can Frozen Transformers in Large Language Models Help with Medical Image Segmentation?)\\n\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review)', 'score': 0.025782676, 'raw_content': None}], 'response_time': 3.91, 'request_id': '52657a33-ab42-4c8f-9b63-3db059ad6a42'}), ToolMessage(content='[{\"title\": \"Jingwen Peng Email & Phone Number | Manulife Lead Analyst ...\", \"url\": \"https://rocketreach.co/jingwen-peng-email_28909123\", \"content\": \"Jingwen Peng, based in Boston, Massachusetts, United States, is currently a Lead Analyst, Investment Analytics at Manulife Investment Management at Manulife\", \"score\": 0.77731717}, {\"title\": \"Jingwen Peng\\'s research works | Guangzhou University and other ...\", \"url\": \"https://www.researchgate.net/scientific-contributions/Jingwen-Peng-2313714327\", \"content\": \"Jingwen Peng\\'s research while affiliated with Guangzhou University and other places.\", \"score\": 0.70702106}, {\"title\": \"Jingwen Peng, CFA - Director - Lead Data Steward at Liberty Mutual ...\", \"url\": \"https://www.linkedin.com/in/jingwen-peng-cfa-3a69b011?trk=public_profile_browsemap\", \"content\": \"Director - Lead Data Steward at Liberty Mutual Investments ¬∑ Experience: Liberty Mutual Investments ¬∑ Location: Boston ¬∑ 500+ connections on LinkedIn.\", \"score\": 0.50065976}, {\"title\": \"90+ \\\\\"Jingwen Peng\\\\\" profiles - LinkedIn\", \"url\": \"https://www.linkedin.com/pub/dir/Jingwen/Peng\", \"content\": \"View the profiles of professionals named \\\\\"Jingwen Peng\\\\\" on LinkedIn. There are 90+ professionals named \\\\\"Jingwen Peng\\\\\", who use LinkedIn to exchange information,\", \"score\": 0.23983903}, {\"title\": \"Repurposing econazole as a pharmacological autophagy inhibitor ...\", \"url\": \"https://pubmed.ncbi.nlm.nih.gov/35865101/\", \"content\": \"### Affiliations\\\\n\\\\n## Abstract [...] ### Add to Collections\\\\n\\\\n### Add to My Bibliography\\\\n\\\\n## Your saved search\\\\n\\\\n## Create a file for external citation management software\\\\n\\\\n## Your RSS Feed\\\\n\\\\n### Full text links\\\\n\\\\nElsevier Science full text link\\\\n\\\\n### Actions\\\\n\\\\n### Page navigation\\\\n\\\\n# Repurposing econazole as a pharmacological autophagy inhibitor to treat pancreatic ductal adenocarcinoma\\\\n\\\\n### Affiliations\\\\n\\\\n# Repurposing econazole as a pharmacological autophagy inhibitor to treat pancreatic ductal adenocarcinoma\\\\n\\\\n### Authors [...] ## LinkOut - more resources\\\\n\\\\n### Full Text Sources\\\\n\\\\n### Miscellaneous\\\\n\\\\nElsevier Science full text link\\\\n\\\\nNCBI Literature Resources\\\\n\\\\nMeSH\\\\nPMC\\\\nBookshelf\\\\nDisclaimer\\\\n\\\\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.\\\\n\\\\nConnect with NLM\\\\n\\\\nNational Library of Medicine  \\\\n8600 Rockville Pike  \\\\nBethesda, MD 20894\\\\n\\\\nWeb Policies  \\\\nFOIA  \\\\nHHS Vulnerability Disclosure\", \"score\": 0.05835133}]', name='tavily_search_results_json', id='4def9384-11c3-4b6f-a706-6a8609a9339c', tool_call_id='call_pb1wdN61FYfLacCinEkwTNBU', artifact={'query': 'Jingwen Peng current affiliation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://rocketreach.co/jingwen-peng-email_28909123', 'title': 'Jingwen Peng Email & Phone Number | Manulife Lead Analyst ...', 'content': 'Jingwen Peng, based in Boston, Massachusetts, United States, is currently a Lead Analyst, Investment Analytics at Manulife Investment Management at Manulife', 'score': 0.77731717, 'raw_content': None}, {'url': 'https://www.researchgate.net/scientific-contributions/Jingwen-Peng-2313714327', 'title': \"Jingwen Peng's research works | Guangzhou University and other ...\", 'content': \"Jingwen Peng's research while affiliated with Guangzhou University and other places.\", 'score': 0.70702106, 'raw_content': None}, {'url': 'https://www.linkedin.com/in/jingwen-peng-cfa-3a69b011?trk=public_profile_browsemap', 'title': 'Jingwen Peng, CFA - Director - Lead Data Steward at Liberty Mutual ...', 'content': 'Director - Lead Data Steward at Liberty Mutual Investments ¬∑ Experience: Liberty Mutual Investments ¬∑ Location: Boston ¬∑ 500+ connections on LinkedIn.', 'score': 0.50065976, 'raw_content': None}, {'url': 'https://www.linkedin.com/pub/dir/Jingwen/Peng', 'title': '90+ \"Jingwen Peng\" profiles - LinkedIn', 'content': 'View the profiles of professionals named \"Jingwen Peng\" on LinkedIn. There are 90+ professionals named \"Jingwen Peng\", who use LinkedIn to exchange information,', 'score': 0.23983903, 'raw_content': None}, {'url': 'https://pubmed.ncbi.nlm.nih.gov/35865101/', 'title': 'Repurposing econazole as a pharmacological autophagy inhibitor ...', 'content': '### Affiliations\\n\\n## Abstract [...] ### Add to Collections\\n\\n### Add to My Bibliography\\n\\n## Your saved search\\n\\n## Create a file for external citation management software\\n\\n## Your RSS Feed\\n\\n### Full text links\\n\\nElsevier Science full text link\\n\\n### Actions\\n\\n### Page navigation\\n\\n# Repurposing econazole as a pharmacological autophagy inhibitor to treat pancreatic ductal adenocarcinoma\\n\\n### Affiliations\\n\\n# Repurposing econazole as a pharmacological autophagy inhibitor to treat pancreatic ductal adenocarcinoma\\n\\n### Authors [...] ## LinkOut - more resources\\n\\n### Full Text Sources\\n\\n### Miscellaneous\\n\\nElsevier Science full text link\\n\\nNCBI Literature Resources\\n\\nMeSH\\nPMC\\nBookshelf\\nDisclaimer\\n\\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.\\n\\nConnect with NLM\\n\\nNational Library of Medicine  \\n8600 Rockville Pike  \\nBethesda, MD 20894\\n\\nWeb Policies  \\nFOIA  \\nHHS Vulnerability Disclosure', 'score': 0.05835133, 'raw_content': None}], 'response_time': 3.38, 'request_id': '1979d7ef-93bf-4c11-8631-23e98916df6c'})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Here‚Äôs what I did and what I found.\\n\\nArXiv search result\\n- Paper: A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\n- Authors: Renjun Xu; Jingwen Peng\\n- Published: 2025-06-14\\n\\nCurrent affiliations (per Tavily searches)\\n\\n- Renjun Xu\\n  - Current affiliation: Zhejiang University\\n  - Position (as listed by Tavily results): Principal Researcher, Zhejiang University\\n  - Notes: Tavily results surfaced OpenReview profiles that show Renjun Xu with the Zhejiang University affiliation. Source snippet: OpenReview profile for Renjun Xu with ‚ÄúPrincipal Researcher, Zhejiang University.‚Äù\\n\\n- Jingwen Peng\\n  - Current affiliation: Manulife Investment Management (Boston, USA)\\n  - Position (as listed by Tavily results): Lead Analyst, Investment Analytics at Manulife Investment Management (at Manulife)\\n  - Notes: Tavily results include a profile indicating Jingwen Peng is based in Boston and works as a Lead Analyst in Investment Analytics at Manulife Investment Management. There are other Jingwen Peng profiles (e.g., Guangzhou University, Liberty Mutual) in other sources, so if you‚Äôd like I can narrow to the specific person by field/location.\\n\\nWould you like me to:\\n- Add direct links to the specific Tavily pages for these affiliations, or\\n- Do a broader pass to confirm with additional sources (e.g., LinkedIn, institutional pages) to corroborate and resolve any name-ambiguity?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2430, 'prompt_tokens': 3288, 'total_tokens': 5718, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2112, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CJ7tAuV4DL4BcAQaERJpEKH4Izuwh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--569a163d-d9af-4a7e-8fb2-2d41918330a3-0', usage_metadata={'input_tokens': 3288, 'output_tokens': 2430, 'total_tokens': 5718, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2112}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the A Comprehensive Survey of Deep Research paper, then search each of the authors to find out where they work now using Tavily!\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzDlZVz1Hnf"
   },
   "source": [
    "#### üèóÔ∏è Activity #2:\n",
    "\n",
    "Please write out the steps the agent took to arrive at the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7c8-Uyarh1v"
   },
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3XeFOT1Sar"
   },
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruQCuzewUuO"
   },
   "source": [
    "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "orYxBZXSxJjZ",
    "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Do you mean a general concept of ‚Äúdeep research‚Äù or a specific thing named \"Deep Research\" (like a company or product)?\\n\\nIf you‚Äôre asking about the concept, here‚Äôs a concise explanation:\\n\\n- Deep research (in-depth, rigorous inquiry) is thorough, systematic investigation aimed at producing reliable, new knowledge or actionable insights.\\n- It goes beyond surface facts and simple summaries. It typically involves:\\n  - A well-defined research question or hypothesis\\n  - A comprehensive literature review and context\\n  - A robust methodology (quantitative, qualitative, or mixed methods)\\n  - Careful data collection and analysis\\n  - Critical evaluation, triangulation, and transparency\\n  - Clear documentation and, when possible, reproducibility or replication\\n- Types of deep research:\\n  - Academic research: generating theories, testing hypotheses, contributing to a field\\n  - Scientific research: experiments, observations, and data analysis to explain natural phenomena\\n  - Market or policy research: deep dives into markets, consumer behavior, or policy impacts\\n  - Investigative journalism: thorough examinations of issues with evidence and corroboration\\n- Key qualities:\\n  - Rigor and methodological soundness\\n  - Attention to bias, ethics, and limitations\\n  - Reproducibility or verifiability of findings\\n  - Critical thinking and careful interpretation of data\\n\\nIf you meant a specific entity named ‚ÄúDeep Research,‚Äù tell me any extra details (industry, location, or a link), and I can pull up targeted information or do a web search for you.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_inputs(input_object):\n",
    "  return {\"messages\" : [HumanMessage(content=input_object[\"text\"])]}\n",
    "\n",
    "def parse_output(input_state):\n",
    "  return {\"answer\" : input_state[\"messages\"][-1].content}\n",
    "\n",
    "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
    "\n",
    "agent_chain_with_formatting.invoke({\"text\" : \"What is Deep Research?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9UkCIqkpyZu"
   },
   "source": [
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
    "\n",
    "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Who were the main authors on the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' paper?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Peng\", \"Xu\"]}   \n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Where do the authors of the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' work now?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Zhejiang\", \"Liberty Mutual\"]}\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMXF2KAsQxs"
   },
   "source": [
    "#### üèóÔ∏è Activity #3:\n",
    "\n",
    "Please create a dataset in the above format with at least 5 questions that pertain to the cohort use-case (more information [here](https://www.notion.so/Session-4-RAG-with-LangGraph-OSS-Local-Models-Eval-w-LangSmith-26acd547af3d80838d5beba464d7e701#26acd547af3d81d08809c9c82a462bdd)), or the use-case you're hoping to tackle in your Demo Day project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "CbagRuJop83E"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Who is the author of the music album 'under my conditions'\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"aimep3\"]}   \n",
    "    },\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"where does the author of the music album 'under my conditions' live?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"nezahualcoyotl\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"How old is the Author of the music album 'under my conditions'?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"36\"]}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7QVFuAmsh7L"
   },
   "source": [
    "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "RLfrZrgSsn85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['5e1e2c52-0862-459b-b11c-62066a6c2582',\n",
       "  '656c8d1e-d1b2-490b-a5aa-2d7f92061933',\n",
       "  'e65b24b8-79db-4581-b660-4a4725abfbb4'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about the cohort use-case to evaluate the Simple Search Agent.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRTXUrTtP9Y"
   },
   "source": [
    "### Task 2: Adding Evaluators\n",
    "\n",
    "Let's use the OpenEvals library to product an evaluator that we can then pass into LangSmith!\n",
    "\n",
    "> NOTE: Examine the `CORRECTNESS_PROMPT` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
      "\n",
      "<Rubric>\n",
      "  A correct answer:\n",
      "  - Provides accurate and complete information\n",
      "  - Contains no factual errors\n",
      "  - Addresses all parts of the question\n",
      "  - Is logically consistent\n",
      "  - Uses precise and accurate terminology\n",
      "\n",
      "  When scoring, you should penalize:\n",
      "  - Factual errors or inaccuracies\n",
      "  - Incomplete or partial answers\n",
      "  - Misleading or ambiguous statements\n",
      "  - Incorrect terminology\n",
      "  - Logical inconsistencies\n",
      "  - Missing key information\n",
      "</Rubric>\n",
      "\n",
      "<Instructions>\n",
      "  - Carefully read the input and output\n",
      "  - Check for factual accuracy and completeness\n",
      "  - Focus on correctness of information rather than style or verbosity\n",
      "</Instructions>\n",
      "\n",
      "<Reminder>\n",
      "  The goal is to evaluate factual correctness and completeness of the response.\n",
      "</Reminder>\n",
      "\n",
      "<input>\n",
      "{inputs}\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "{outputs}\n",
      "</output>\n",
      "\n",
      "Use the reference outputs below to help you evaluate the correctness of the response:\n",
      "\n",
      "<reference_outputs>\n",
      "{reference_outputs}\n",
      "</reference_outputs>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "print(CORRECTNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "QrAUXMFftlAY"
   },
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge\n",
    "\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"openai:o3-mini\", # very impactful to the final score\n",
    "        feedback_key=\"correctness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a custom Evaluator for our created dataset above - we do this by first making a simple Python function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "  # determine if the phrases in the reference_outputs are in the outputs\n",
    "  required = reference_outputs.get(\"must_mention\") or []\n",
    "  score = all(phrase in outputs[\"answer\"] for phrase in required)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNtHORUh0jZY"
   },
   "source": [
    "#### ‚ùì Question #4:\n",
    "\n",
    "What are some ways you could improve this metric as-is?\n",
    "\n",
    "> NOTE: Alternatively you can suggest where gaps exist in this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJr349zhv7"
   },
   "source": [
    "Task 3: Evaluating\n",
    "\n",
    "All that is left to do is evaluate our agent's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "efcf57067cf743d8b4ce059a61cbe02e",
      "53e33aae3b97490c82aec7bbb0d6ebba",
      "ad84e0e971d3455db2efe7dd0d1f803e",
      "72adef9b70dd48198b7322b6c5b113cf",
      "8a61d045ffd44ac58f3f13eb10044836",
      "041e22a9b5514e36bd4d1dac01d5d398",
      "886d762f2a7c421382efb5502c6d42a1",
      "ab91fd625bbd43afbf8c6398193a88d0",
      "716557ad09874dcb989d75f7c74424cd",
      "77d4c0ebaae045b58efc4f789c9a2360",
      "0d622ccc56264fac8fd7508dbdbe6e29"
     ]
    },
    "id": "p5TeCUUkuGld",
    "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'simple_agent, baseline-662ceee8' at:\n",
      "https://smith.langchain.com/o/9caeceb1-c4f0-4b46-af6b-bedb25ad4d93/datasets/68856e4e-852f-45dd-a608-18febd11eb77/compare?selectedSessions=88ed3726-b0d5-44f4-bb08-ac4f6a21e6b5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b28fd5512724558bf64922ddde19b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = client.evaluate(\n",
    "    agent_chain_with_formatting,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correctness_evaluator, must_mention],\n",
    "    experiment_prefix=\"simple_agent, baseline\",  # optional, experiment name prefix\n",
    "    description=\"Testing the baseline system.\",  # optional, experiment description\n",
    "    max_concurrency=4, # optional, add concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTNe4kWrplB"
   },
   "source": [
    "## Part 2: LangGraph with Helpfulness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1wKRddbIY_S"
   },
   "source": [
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
    "\n",
    "We're going to make a few key adjustments to account for this:\n",
    "\n",
    "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
    "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npTYJ8ayR5B3"
   },
   "source": [
    "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-LQ84YhyJG0w"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD7EV0HqSQcb"
   },
   "source": [
    "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajBwLkFVi1N"
   },
   "source": [
    "#### üèóÔ∏è Activity #4:\n",
    "\n",
    "Please write markdown for the following cells to explain what each is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6rN7feNVn9f"
   },
   "source": [
    "This code is creating a LangGraph state machine for an AI agent with a helpfulness check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6r6XXA5FJbVf",
    "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110dad480>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ22o2mWVrfp"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNWHwWxuRiLY",
    "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110dad480>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsXeF6xlaXOZ"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "z_Sq3A9SaV1O"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  initial_query = state[\"messages\"][0]\n",
    "  final_response = state[\"messages\"][-1]\n",
    "\n",
    "  if len(state[\"messages\"]) > 10:\n",
    "    return \"END\"\n",
    "\n",
    "  prompt_template = \"\"\"\\\n",
    "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
    "\n",
    "  Initial Query:\n",
    "  {initial_query}\n",
    "\n",
    "  Final Response:\n",
    "  {final_response}\"\"\"\n",
    "\n",
    "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "  helpfulness_check_model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
    "\n",
    "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
    "\n",
    "  if \"Y\" in helpfulness_response:\n",
    "    return \"end\"\n",
    "  else:\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BhnBW2YVsJO"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVTKnWMbP_8T",
    "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110dad480>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"action\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGDLEWOIVtK0"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbDK2MbuREgU",
    "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x110dad480>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSI8AOaEVvT-"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "oQldl8ERQ8lf"
   },
   "outputs": [],
   "source": [
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F67FGCMRVwGz"
   },
   "source": [
    "##### YOUR MARKDOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3oo8E-PRK1T",
    "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Deep Research Agents are AI-powered systems designed to perform thorough, autonomous research tasks. Instead of just returning search results, they plan, gather, evaluate, and synthesize information from many sources to produce structured, cited outputs such as literature reviews, reports, or data analyses.\\n\\nKey ideas:\\n- Autonomous research: They set a goal, plan the steps, and carry them out with minimal human intervention, while often keeping the human in the loop for review.\\n- Multisource gathering: They pull information from diverse sources (academic papers, news, databases, patents, websites) and reconcile conflicting findings.\\n- Critical evaluation: They assess source credibility, bias, and evidence strength, not just extract quotes.\\n- Synthesis and output: They produce cohesive narratives, summaries, datasets, or decision-ready reports with traceable citations and provenance.\\n\\nHow they typically work (high level):\\n- Define scope and goals: What question to answer or what problem to solve.\\n- Plan actions: Decide which sources to query, what data to extract, and what analyses to perform.\\n- Retrieve and extract: Use web access, databases, APIs, or arXiv-like sources to collect information.\\n- Evaluate sources: Check credibility, recency, authorship, and potential biases.\\n- Synthesize: Integrate findings across sources, resolve contradictions, and highlight key insights.\\n- Output with provenance: Generate a structured report or summary that includes citations and, if needed, raw data or code.\\n- Review loop: Optional human review to validate conclusions or request refinements.\\n\\nCapabilities you might see:\\n- Literature reviews and meta-analyses\\n- Comprehensive market or policy research\\n- Technical literature synthesis with citations\\n- Patent landscape analyses\\n- Data extraction and preprocessing from papers or reports\\n- Trend detection and gap analysis\\n\\nCommon components and tech:\\n- Large language model plus retrieval-augmented generation (RAG)\\n- Tools for web browsing, database access, arXiv or other repositories\\n- Memory and indexing for long-running searches (to maintain context over many steps)\\n- Source credibility and citation management modules\\n- Workflow/planning and action libraries to automate tasks\\n\\nUse cases:\\n- Academia: rapid literature reviews, methodological scoping, and synthesis for theses or grant proposals.\\n- Industry: market intelligence, competitive analysis, technology scouting.\\n- Policy and journalism: evidence synthesis, risk assessment, and fact-checking.\\n- Healthcare and science: systematic reviews, protocol comparisons, and evidence mapping (with rigorous citations).\\n\\nChallenges and cautions:\\n- Hallucinations and outdated info: verify against primary sources.\\n- Bias and selective reporting: ensure diverse sources and critical appraisal.\\n- Access and rights: respect paywalls, licensing, and data use policies.\\n- Reproducibility: keep transparent pipelines and provide sources, methods, and data when possible.\\n- Safety and ethics: avoid misrepresentation of findings and disclose limitations.\\n\\nHow it differs from a simple search:\\n- Simple search surfaces results; a Deep Research Agent aims to produce a reasoned, sourced synthesis with structured outputs and traceable provenance.\\n- It often operates iteratively, refining questions and digging deeper based on findings, rather than delivering a single-pass answer.\\n\\nIf you‚Äôd like, tell me your domain or goal (e.g., a literature review on a topic, market research, or a patent landscape). I can tailor a more detailed explanation or sketch out a high-level architecture for a Deep Research Agent in that domain.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1914, 'prompt_tokens': 239, 'total_tokens': 2153, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CJ7zi0siPGVVxnIlZcL5Myp0hbVdf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3e84c0d1-bb5a-4695-9ff6-bd437e3e7dc3-0', usage_metadata={'input_tokens': 239, 'output_tokens': 1914, 'total_tokens': 2153, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"What are Deep Research Agents?\")]}\n",
    "\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVmZPs6lnpsM"
   },
   "source": [
    "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "### Task 4: Helpfulness Check of Gen AI Pattern Descriptions\n",
    "\n",
    "Let's ask our system about the 3 main patterns in Generative AI:\n",
    "\n",
    "1. Context Engineering\n",
    "2. Fine-tuning\n",
    "3. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ZoLl7GlXoae-"
   },
   "outputs": [],
   "source": [
    "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkh0YJuCp3Zl",
    "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer\n",
      "- What it is: Context engineering is the discipline of designing and managing the entire information environment that an AI model (especially a large language model) uses to perform a task. It goes beyond prompt crafting to orchestrate all inputs the model sees‚Äîsystem instructions, user queries, memory (short- and long-term), retrieved documents, tools, schemas, and the task context‚Äîso the model can work reliably and effectively.\n",
      "- When it ‚Äúhit the scene‚Äù: The term started gaining real traction around 2023 and surged through 2024‚Äì2025. It‚Äôs often traced to prominent industry voices (e.g., Andrej Karpathy) and became popular in practitioner-focused writing and tooling (LangChain, DataCamp, prompting guides, etc.).\n",
      "\n",
      "More detail\n",
      "- How it differs from prompt engineering\n",
      "  - Prompt engineering focuses on crafting a single, well-phrased input to elicit a good response.\n",
      "  - Context engineering treats the model as part of a dynamic system and designs the broader context window and information ecosystem (memory, history, tools, data sources, retrieved documents, schemas) that the model uses over multiple steps or interactions.\n",
      "\n",
      "- What it typically involves\n",
      "  - System prompts or instructions that shape behavior across a session.\n",
      "  - Management of short-term memory (conversation history) and long-term memory (personas, user profiles, institutional knowledge).\n",
      "  - Retrieval-augmented data, documents, APIs, tools, and their formatting.\n",
      "  - Data schemas, constraints, and governance/verification hooks to keep outputs trustworthy.\n",
      "  - Orchestration across multiple steps or ‚Äúsubagents‚Äù to gather and supply the right context at the right time.\n",
      "\n",
      "If you‚Äôd like, I can pull a few representative sources (articles, blog posts) that helped popularize the term and map a more precise timeline.\n",
      "\n",
      "\n",
      "\n",
      "Short answer\n",
      "- Fine-tuning is taking a model that‚Äôs already been trained on a large general dataset and continuing training it on a smaller, task-specific dataset to adapt it to a particular job (e.g., sentiment analysis, translation). You usually use a smaller learning rate and often adjust only parts of the model.\n",
      "\n",
      "When did it break onto the scene?\n",
      "- The idea existed earlier in general transfer learning, but fine-tuning as a practical, mainstream approach especially for NLP really took off around 2018‚Äì2019.\n",
      "  - 2018: ULMFiT (Howard & Ruder) popularized fine-tuning a pre-trained language model for NLP tasks.\n",
      "  - 2018: BERT (Devlin et al.) showed how simple task-specific heads + fine-tuning of a pre-trained transformer could achieve state-of-the-art results across many tasks.\n",
      "  From there, fine-tuning became the standard way to adapt large pre-trained models to new tasks, continuing to grow with GPT-2/3, T5, and beyond.\n",
      "\n",
      "How it‚Äôs typically done (high level)\n",
      "1) Start with a pre-trained model (e.g., a language model or a Transformer like BERT, T5).\n",
      "2) Prepare task-specific labeled data (and/or unlabeled data for additional pre-training).\n",
      "3) Replace or append the task-specific output head (e.g., classification layer).\n",
      "4) Fine-tune the model on the task data, usually with:\n",
      "   - A smaller learning rate than you‚Äôd use from scratch.\n",
      "   - Possible layer freezing or discriminative learning rates (lower LR for earlier layers, higher for later layers).\n",
      "   - Sometimes gradual unfreezing (unfreeze layers one by one).\n",
      "5) Evaluate and adjust (regularization, early stopping, etc.).\n",
      "\n",
      "Variants and related ideas\n",
      "- Adapter-tuning: keep the main model fixed and train small additional modules (adapters) to reduce compute and memory.\n",
      "- Prompt-tuning / soft prompts: adjust only small prompt tokens or input conditioning instead of the whole model.\n",
      "- Task-specific heads + data augmentation or balancing to improve performance.\n",
      "\n",
      "Common pitfalls and tips\n",
      "- Catastrophic forgetting: fine-tuning too aggressively can lose generality; use lower learning rates and consider gradual unfreezing.\n",
      "- Data mismatch: ensure your fine-tuning data distribution matches your intended use; domain mismatch hurts performance.\n",
      "- Overfitting on small datasets: use regularization, dropout, early stopping, and possibly cross-validation.\n",
      "- Computational cost: large models are expensive to fine-tune; consider parameter-efficient methods (adapters, prefix-tuning) if resources are limited.\n",
      "- Evaluation realism: validate on truly held-out data that reflects real-world use.\n",
      "\n",
      "If you tell me your domain (e.g., NLP, vision, or a specific task) and what resources you have (compute, data size), I can tailor a concrete fine-tuning plan and point you to key papers.\n",
      "\n",
      "\n",
      "\n",
      "Short answer:\n",
      "- LLM-based agents are systems built around a large language model (LLM) that can plan, reason, and act by calling external tools (APIs, apps, code execution, web access, etc.) to achieve user-defined goals. They‚Äôre essentially autonomous or semi-autonomous assistant programs that use an LLM as their core brain plus a toolkit of actions.\n",
      "\n",
      "More detail:\n",
      "- What they are:\n",
      "  - An AI agent whose central decision-maker is an LLM.\n",
      "  - They don‚Äôt just chat; they decide a sequence of actions, call tools, handle results, and repeat until a goal is met.\n",
      "  - They often have components like memory/state, a planner, a tool-catching layer (to call APIs or code), and a safety/guardrail layer.\n",
      "\n",
      "- How they work (high level):\n",
      "  - Receive a goal from the user.\n",
      "  - The LLM plans a sequence of steps (reasoning plus action).\n",
      "  - The agent calls external tools or APIs to perform actions (search, fetch data, run code, automate apps).\n",
      "  - It processes results, possibly revises its plan, and continues until done.\n",
      "  - It may store context for later sessions (short/long-term memory) and apply safety checks.\n",
      "\n",
      "- What makes them different from a normal chat with an LLM:\n",
      "  - They actively act in the world (via tools) rather than just generating text.\n",
      "  - They perform multi-step tasks autonomously and can loop back to adjust plans.\n",
      "  - They can integrate across apps and data sources to complete complex tasks.\n",
      "\n",
      "- When they ‚Äúbroke onto the scene‚Äù:\n",
      "  - The concept gained major visibility in 2023‚Äì2024. Key milestones:\n",
      "    - 2023: Publicly demonstrated capabilities of LLMs to use tools via function calling, plugins, and structured tool use (e.g., GPT-4 with tools). This spurred ‚Äúagent‚Äù frameworks.\n",
      "    - 2023: Popular open-source and semi-open initiatives like Auto-GPT and the LangChain ecosystem popularized autonomous, tool-using agents.\n",
      "    - 2023‚Äì2024: Researchers and developers published influential methods (e.g., ReAct prompting, Toolformer) and more robust agent architectures, accelerating mainstream adoption.\n",
      "  - So, while the idea existed in AI for a while, the LLM-based agent paradigm became widely recognized and talked about around 2023.\n",
      "\n",
      "- Common examples and terms you might hear:\n",
      "  - Auto-GPT, BabyAGI (early prototypes of automated agents)\n",
      "  - LangChain (tools and agent frameworks)\n",
      "  - ReAct (a prompting approach combining reasoning and actions)\n",
      "  - Toolformer (method for LLMs to decide when/how to use tools)\n",
      "  - GPT-4 with plugins/tools (official ecosystem enabling tool use)\n",
      "\n",
      "- Practical cautions:\n",
      "  - Reliability and safety: agents can hallucinate, mishandle data, or take unintended actions if not well guarded.\n",
      "  - Tool integration: effectiveness depends on how well tools are wired and monitored.\n",
      "  - Not a single product: ‚ÄúLLM-based agent‚Äù is a family of approaches and architectures, not one standard implementation.\n",
      "\n",
      "If you want, I can tailor this to a specific domain (e.g., software automation, customer support, research assistants) or point you to a few foundational papers and tutorials.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pattern in patterns:\n",
    "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
    "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
    "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "  print(messages[\"messages\"][-1].content)\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
